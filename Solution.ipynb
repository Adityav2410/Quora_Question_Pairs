{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# reload(sys)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from pdb import set_trace as bp\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import sys \n",
    "from sklearn.utils import shuffle\n",
    "from dataGenerator import DataGenerator\n",
    "from models import QuoraModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from utils import plotGraph,plotLengthHistogram,plotHist,labelPlot\n",
    "from utils import createTiles, plotHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, LSTM,TimeDistributed, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class QuoraModel:\n",
    "    \n",
    "    def __init__(self, timeSlice = 2, questionLen = 25, wordEmbeddDim = 300, dataPath = './model', weightFile = 'model.h5' ):\n",
    "        self.timeSlice = timeSlice\n",
    "        self.questionLen = questionLen\n",
    "        self.wordEmbeddDim = wordEmbeddDim\n",
    "        self.dataPath = dataPath\n",
    "        self.model_best_loss = 100\n",
    "        self.model_best_acc =  0\n",
    "        self.weightFilePath = os.path.join(dataPath, weightFile )\n",
    "        \n",
    "    def getCallbackList(self, model_name = 'model1'):\n",
    "        fileName= model_name + \"_weights-{epoch:02d}-{val_loss:.3f}-{val_acc:.2f}.hdf5\"\n",
    "        filePath = os.path.join(self.dataPath, fileName)\n",
    "        checkpoint = ModelCheckpoint(filePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        return callbacks_list\n",
    "        \n",
    "    def getModel1(self, load_weight = True):\n",
    "        layer1Dim = 500\n",
    "        layer2Dim = 250\n",
    "        nDense1   = 256\n",
    "        nDense2   = 128\n",
    "        inputs = Input(shape=(self.timeSlice,self.questionLen, self.wordEmbeddDim))\n",
    "        batchNorm1 = BatchNormalization(name = \"batch_norm_1\")(inputs)\n",
    "        lstm1 = TimeDistributed(LSTM(layer1Dim, activation = 'relu', return_sequences = True, dropout = 0.3), name = 'lstm_1')(batchNorm1)\n",
    "        batchNorm2 = BatchNormalization( name = \"batch_norm_2\")(lstm1)\n",
    "        lstm2 = TimeDistributed(LSTM(layer2Dim, activation = 'relu', return_sequences = False), name = 'lstm_2')(batchNorm2)\n",
    "        flatten = Flatten(name = 'flatten')(lstm2)\n",
    "        batchNorm3 = BatchNormalization(name = \"batch_norm_3\")(flatten)\n",
    "        dense1 = Dense(nDense1, activation = 'relu')(batchNorm3)\n",
    "        batchNorm4 = BatchNormalization(name = \"batch_norm_4\")(dense1)\n",
    "        dense2 = Dense(nDense2, activation = 'relu')(batchNorm4)\n",
    "        output = Dense(1, activation = 'sigmoid')(dense2)\n",
    "        self.model = Model(inputs = inputs, outputs = output)\n",
    "        if load_weight:\n",
    "            if os.path.isfile(self.weightFilePath) :\n",
    "                try:\n",
    "                    self.model.load_weights(self.weightFilePath)\n",
    "                    print(\"Weights loaded\")\n",
    "                except:\n",
    "                    print(\"Failed to load weights\")\n",
    "                    pass\n",
    "        return self.model\n",
    "\n",
    "    \n",
    "    def getModel2(self):\n",
    "        layer1Dim = 500\n",
    "        layer2Dim = 250\n",
    "        num_dense   = 100\n",
    "        rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "        rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "        act = 'relu'\n",
    "        bp()\n",
    "        lstm_layer = LSTM(layer1Dim)\n",
    "\n",
    "        sequence_1_input = Input(shape=(self.questionLen, self.wordEmbeddDim), dtype='int32')\n",
    "        x1 = lstm_layer(sequence_1_input)\n",
    "        \n",
    "        sequence_2_input = Input(shape=(self.questionLen, self.wordEmbeddDim), dtype='int32')\n",
    "        x2 = lstm_layer(sequence_2_input)\n",
    "\n",
    "        merged = concatenate([x1, x2])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "        self.model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                           outputs=preds)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def saveWeights(self):\n",
    "#         try:\n",
    "#             self.model.save_weights(self.weightFilePath)\n",
    "#             print(\"Weights successfully saved\")\n",
    "#         except:\n",
    "#             print(\"Exception: Weight save failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "timeSlice = 2\n",
    "questionLen = 25\n",
    "wordEmbeddDim = 100\n",
    "batchSize  = 16\n",
    "nEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generator succesfully initialized :) :) :) \n"
     ]
    }
   ],
   "source": [
    "dataGenerator = DataGenerator(batchSize = batchSize, \n",
    "                              questionLen = questionLen,\n",
    "                              wordEmbeddDim = wordEmbeddDim)\n",
    "nData = dataGenerator.nTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 2, 25, 100)        0         \n",
      "_________________________________________________________________\n",
      "batch_norm_1 (BatchNormaliza (None, 2, 25, 100)        400       \n",
      "_________________________________________________________________\n",
      "lstm_1 (TimeDistributed)     (None, 2, 25, 500)        1202000   \n",
      "_________________________________________________________________\n",
      "batch_norm_2 (BatchNormaliza (None, 2, 25, 500)        2000      \n",
      "_________________________________________________________________\n",
      "lstm_2 (TimeDistributed)     (None, 2, 250)            751000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "batch_norm_3 (BatchNormaliza (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "batch_norm_4 (BatchNormaliza (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,119,705\n",
      "Trainable params: 2,116,993\n",
      "Non-trainable params: 2,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This returns a tensor\n",
    "net = QuoraModel(questionLen = questionLen,\n",
    "                   wordEmbeddDim = wordEmbeddDim)\n",
    "model = net.getModel1(load_weight = False)\n",
    "nadam = Nadam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(optimizer=nadam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.693\t BessAcc: 0.512 \t Saving weights \n",
      "Epoch:0\t Iter: 1\t TrainLoss 0.6926\t ValidLos: 0.6934             TrainAcc: 0.6250\t ValidAcc: 0.5125\n",
      "Epoch:0\t Iter: 11\t TrainLoss 0.6848\t ValidLos: 0.6935             TrainAcc: 0.7500\t ValidAcc: 0.5250\n",
      "Best loss:0.689\t BessAcc: 0.600 \t Saving weights \n",
      "Epoch:0\t Iter: 21\t TrainLoss 0.6779\t ValidLos: 0.6885             TrainAcc: 0.7500\t ValidAcc: 0.6000\n",
      "Best loss:0.684\t BessAcc: 0.613 \t Saving weights \n",
      "Epoch:0\t Iter: 31\t TrainLoss 0.6729\t ValidLos: 0.6844             TrainAcc: 0.7500\t ValidAcc: 0.6125\n",
      "Best loss:0.680\t BessAcc: 0.650 \t Saving weights \n",
      "Epoch:0\t Iter: 41\t TrainLoss 0.6695\t ValidLos: 0.6799             TrainAcc: 0.7500\t ValidAcc: 0.6500\n",
      "Epoch:0\t Iter: 51\t TrainLoss 0.6660\t ValidLos: 0.6834             TrainAcc: 0.7500\t ValidAcc: 0.6125\n",
      "Epoch:0\t Iter: 61\t TrainLoss 0.6627\t ValidLos: 0.6800             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:0\t Iter: 71\t TrainLoss 0.6607\t ValidLos: 0.6869             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Best loss:0.678\t BessAcc: 0.625 \t Saving weights \n",
      "Epoch:0\t Iter: 81\t TrainLoss 0.6593\t ValidLos: 0.6779             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Best loss:0.658\t BessAcc: 0.738 \t Saving weights \n",
      "Epoch:0\t Iter: 91\t TrainLoss 0.6563\t ValidLos: 0.6579             TrainAcc: 0.7500\t ValidAcc: 0.7375\n",
      "Epoch:1\t Iter: 1\t TrainLoss 0.6535\t ValidLos: 0.6612             TrainAcc: 0.7500\t ValidAcc: 0.7125\n",
      "Epoch:1\t Iter: 11\t TrainLoss 0.6506\t ValidLos: 0.6893             TrainAcc: 0.7500\t ValidAcc: 0.5625\n",
      "Epoch:1\t Iter: 21\t TrainLoss 0.6476\t ValidLos: 0.6937             TrainAcc: 0.7500\t ValidAcc: 0.5375\n",
      "Epoch:1\t Iter: 31\t TrainLoss 0.6445\t ValidLos: 0.6710             TrainAcc: 0.7500\t ValidAcc: 0.6500\n",
      "Epoch:1\t Iter: 41\t TrainLoss 0.6404\t ValidLos: 0.6790             TrainAcc: 0.7500\t ValidAcc: 0.6000\n",
      "Epoch:1\t Iter: 51\t TrainLoss 0.6387\t ValidLos: 0.6858             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Epoch:1\t Iter: 61\t TrainLoss 0.6354\t ValidLos: 0.6894             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Best loss:0.652\t BessAcc: 0.688 \t Saving weights \n",
      "Epoch:1\t Iter: 71\t TrainLoss 0.6331\t ValidLos: 0.6523             TrainAcc: 0.7500\t ValidAcc: 0.6875\n",
      "Epoch:1\t Iter: 81\t TrainLoss 0.6287\t ValidLos: 0.6952             TrainAcc: 0.7500\t ValidAcc: 0.5500\n",
      "Best loss:0.639\t BessAcc: 0.713 \t Saving weights \n",
      "Epoch:1\t Iter: 91\t TrainLoss 0.6265\t ValidLos: 0.6385             TrainAcc: 0.7500\t ValidAcc: 0.7125\n",
      "Epoch:2\t Iter: 1\t TrainLoss 0.6236\t ValidLos: 0.6511             TrainAcc: 0.7500\t ValidAcc: 0.6750\n",
      "Epoch:2\t Iter: 11\t TrainLoss 0.6200\t ValidLos: 0.6548             TrainAcc: 0.7500\t ValidAcc: 0.6750\n",
      "Epoch:2\t Iter: 21\t TrainLoss 0.6171\t ValidLos: 0.6725             TrainAcc: 0.7500\t ValidAcc: 0.5875\n",
      "Epoch:2\t Iter: 31\t TrainLoss 0.6123\t ValidLos: 0.6664             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:2\t Iter: 41\t TrainLoss 0.6089\t ValidLos: 0.6602             TrainAcc: 0.7500\t ValidAcc: 0.6500\n",
      "Epoch:2\t Iter: 51\t TrainLoss 0.6052\t ValidLos: 0.6714             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:2\t Iter: 61\t TrainLoss 0.6033\t ValidLos: 0.7117             TrainAcc: 0.7500\t ValidAcc: 0.5500\n",
      "Epoch:2\t Iter: 71\t TrainLoss 0.6023\t ValidLos: 0.6636             TrainAcc: 0.7500\t ValidAcc: 0.6500\n",
      "Epoch:2\t Iter: 81\t TrainLoss 0.6012\t ValidLos: 0.6745             TrainAcc: 0.7500\t ValidAcc: 0.6000\n",
      "Epoch:2\t Iter: 91\t TrainLoss 0.6000\t ValidLos: 0.7133             TrainAcc: 0.7500\t ValidAcc: 0.5375\n",
      "Best loss:0.628\t BessAcc: 0.688 \t Saving weights \n",
      "Epoch:3\t Iter: 1\t TrainLoss 0.5988\t ValidLos: 0.6285             TrainAcc: 0.7500\t ValidAcc: 0.6875\n",
      "Epoch:3\t Iter: 11\t TrainLoss 0.5962\t ValidLos: 0.6688             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:3\t Iter: 21\t TrainLoss 0.5933\t ValidLos: 0.6783             TrainAcc: 0.7500\t ValidAcc: 0.5875\n",
      "Epoch:3\t Iter: 31\t TrainLoss 0.5900\t ValidLos: 0.7083             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Epoch:3\t Iter: 41\t TrainLoss 0.5868\t ValidLos: 0.6615             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:3\t Iter: 51\t TrainLoss 0.5832\t ValidLos: 0.6462             TrainAcc: 0.7500\t ValidAcc: 0.6875\n",
      "Epoch:3\t Iter: 61\t TrainLoss 0.5745\t ValidLos: 0.6428             TrainAcc: 0.7500\t ValidAcc: 0.6625\n",
      "Epoch:3\t Iter: 71\t TrainLoss 0.5665\t ValidLos: 0.7629             TrainAcc: 0.7500\t ValidAcc: 0.5000\n",
      "Epoch:3\t Iter: 81\t TrainLoss 0.5575\t ValidLos: 0.7574             TrainAcc: 0.7500\t ValidAcc: 0.4750\n",
      "Epoch:3\t Iter: 91\t TrainLoss 0.5514\t ValidLos: 0.6978             TrainAcc: 0.7500\t ValidAcc: 0.5875\n",
      "Epoch:4\t Iter: 1\t TrainLoss 0.5419\t ValidLos: 0.7068             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Epoch:4\t Iter: 11\t TrainLoss 0.5328\t ValidLos: 0.7153             TrainAcc: 0.7500\t ValidAcc: 0.6000\n",
      "Epoch:4\t Iter: 21\t TrainLoss 0.5254\t ValidLos: 0.7885             TrainAcc: 0.7500\t ValidAcc: 0.5500\n",
      "Epoch:4\t Iter: 31\t TrainLoss 0.5142\t ValidLos: 0.7667             TrainAcc: 0.7500\t ValidAcc: 0.5000\n",
      "Epoch:4\t Iter: 41\t TrainLoss 0.4999\t ValidLos: 0.7572             TrainAcc: 0.7500\t ValidAcc: 0.5125\n",
      "Epoch:4\t Iter: 51\t TrainLoss 0.4850\t ValidLos: 0.7185             TrainAcc: 0.7500\t ValidAcc: 0.5875\n",
      "Epoch:4\t Iter: 61\t TrainLoss 0.4661\t ValidLos: 0.7496             TrainAcc: 0.7500\t ValidAcc: 0.5750\n",
      "Epoch:4\t Iter: 71\t TrainLoss 0.4468\t ValidLos: 0.6967             TrainAcc: 0.7500\t ValidAcc: 0.6250\n",
      "Epoch:4\t Iter: 81\t TrainLoss 0.4299\t ValidLos: 0.6813             TrainAcc: 0.7500\t ValidAcc: 0.6375\n",
      "Epoch:4\t Iter: 91\t TrainLoss 0.4146\t ValidLos: 0.7159             TrainAcc: 0.8125\t ValidAcc: 0.6000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-710b6c0a013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValidBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mvalidLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidLoss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbestLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mbestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aditya/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainHist = [[], []]\n",
    "validHist = [[], []]\n",
    "iterSize = 100\n",
    "bestLoss = 100\n",
    "weightFilePath = \"./1234\"\n",
    "nIter = dataGenerator.nTrainData/(batchSize)\n",
    
    "for i in range(nEpochs):\n",
    "    for j in range(nIter):\n",
    "        trainX, trainY = dataGenerator.getTrainBatch()\n",
    "        model.fit( trainX, trainY, verbose = False )\n",
    "        if( j%iterSize == 0 ):\n",
    "            trainX, trainY = dataGenerator.getTrainBatch(batchSize = 5*batchSize)\n",
    "            validX, validY = dataGenerator.getValidBatch(batchSize = 5*batchSize)\n",
    "            trainLoss, trainAcc = model.evaluate( trainX, trainY, verbose = False )\n",
    "            validLoss, validAcc = model.evaluate(validX, validY, verbose = False )\n",
    "            if validLoss < bestLoss:\n",
    "                bestLoss = validLoss\n",
    "                print(\"Best loss:{0:.3f}\\t BessAcc: {1:.3f} \\t Saving weights \".format(validLoss, validAcc))\n",
    "                if os.path.isfile(weightFilePath):\n",
    "                    os.remove(weightFilePath)\n",
    "                weightFilePath = 'model/weights_loss_{0:.4f}_acc_{1:.3f}.h5py'.format(validLoss, validAcc)\n",
    "                model.save_weights(weightFilePath)\n",
    "            print( \"Epoch:{0}\\t Iter: {1}\\t TrainLoss {2:.4f}\\t ValidLos: {3:.4f} \\\n",
    "            TrainAcc: {4:.4f}\\t ValidAcc: {5:.4f}\"\n",
    "                   .format(i, j+1, trainLoss, validLoss, trainAcc, validAcc))\n",
    "            trainHist[0].append(trainLoss)\n",
    "            trainHist[1].append(trainAcc)\n",
    "            validHist[0].append(validLoss)\n",
    "            validHist[1].append(validAcc)\n",
    "plotHistory(trainHist, validHist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainHist = [[], []]\n",
    "# validHist = [[], []]\n",
    "# model2 = False\n",
    "# nIter = dataGenerator.nTrainData/(batchSize)\n",
    "# for i in range(nEpochs):\n",
    "#     for j in range(nIter):\n",
    "#         trainX, trainY = dataGenerator.getTrainBatch()\n",
    "#         if model2:\n",
    "#             trainX = [trainX[:,0], trainX[:,1]]\n",
    "#         model.fit( trainX, trainY, verbose = False )\n",
    "#         if( j%iterSize == 0 ):\n",
    "#             trainX, trainY = dataGenerator.getTrainBatch(batchSize = 5*batchSize)\n",
    "#             validX, validY = dataGenerator.getValidBatch(batchSize = 5*batchSize)\n",
    "#             if model2:\n",
    "#                 trainX = [trainX[:,0], trainX[:,1]]\n",
    "#                 validX = [validX[:,0], validX[:,1]]\n",
    "#             trainLoss, trainAcc = model.evaluate( trainX, trainY, verbose = False )\n",
    "#             validLoss, validAcc = model.evaluate(validX, validY, verbose = False )\n",
    "#             print( \"Epoch:{0}\\t Iter: {1}\\t TrainLoss {2:.4f}\\t ValidLos: {3:.4f} \\\n",
    "#             TrainAcc: {4:.4f}\\t ValidAcc: {5:.4f}\"\n",
    "#                    .format(i, j+1, trainLoss, validLoss, trainAcc, validAcc))\n",
    "#             trainHist[0].append(trainLoss)\n",
    "#             trainHist[1].append(trainAcc)\n",
    "#             validHist[0].append(validLoss)\n",
    "#             validHist[1].append(validAcc)\n",
    "#     model.save_weights('data/model.h5')\n",
    "# plotHistory(trainHist, validHist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
